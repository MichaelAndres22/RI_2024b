{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "91b170b1-2397-4990-9c42-c752dc9ce3e8",
   "metadata": {},
   "source": [
    "## **Trabajo en clase: TOPIC MODELING**  \n",
    "**Michael Pillaga**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2110b81f-8285-4d7d-aade-a0343f3d69de",
   "metadata": {},
   "source": [
    "### 1. Lectura del archivo CSV y visualización inicial  \n",
    "Lee el archivo CSV que contiene los datos del podcast, como el `id`, el `guest`, el `title` y las transcripciones (`text`). Luego, muestra las primeras 5 filas del DataFrame para confirmar que los datos se cargaron correctamente.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7729c7a7-c54d-4c79-ad23-127eee530046",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   id            guest                    title  \\\n",
      "0   1      Max Tegmark                 Life 3.0   \n",
      "1   2    Christof Koch            Consciousness   \n",
      "2   3    Steven Pinker  AI in the Age of Reason   \n",
      "3   4    Yoshua Bengio            Deep Learning   \n",
      "4   5  Vladimir Vapnik     Statistical Learning   \n",
      "\n",
      "                                                text  \n",
      "0  As part of MIT course 6S099, Artificial Genera...  \n",
      "1  As part of MIT course 6S099 on artificial gene...  \n",
      "2  You've studied the human mind, cognition, lang...  \n",
      "3  What difference between biological neural netw...  \n",
      "4  The following is a conversation with Vladimir ...  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('./podcastdata_dataset.csv')\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7310bf43-f1c9-44ac-9ddf-3b8c01b45e0e",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 2. División del texto en bloques de oraciones  \n",
    "Se define una función `agrupar_oraciones` para dividir el texto de cada transcripción en bloques de 200 oraciones (o el tamaño especificado). Luego, se aplica esta función a la columna `text` del DataFrame `df`.  \n",
    "Los bloques generados se expanden en un nuevo DataFrame (`bloques_df`), manteniendo información relevante como el `id`, el `guest` y el `title` del episodio correspondiente.  \n",
    "Finalmente, se muestra una vista previa de los primeros bloques generados.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3d60c059-d130-45f5-be02-98f7599ccf9d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   id          guest          title  \\\n",
      "0   1    Max Tegmark       Life 3.0   \n",
      "1   1    Max Tegmark       Life 3.0   \n",
      "2   1    Max Tegmark       Life 3.0   \n",
      "3   1    Max Tegmark       Life 3.0   \n",
      "4   2  Christof Koch  Consciousness   \n",
      "\n",
      "                                        bloque_texto  \n",
      "0  As part of MIT course 6S099, Artificial Genera...  \n",
      "1   The way they get AGI is building a quantum co...  \n",
      "2   So there's, it's almost like a sea that's ris...  \n",
      "3   And that kind of information helps to build t...  \n",
      "4  As part of MIT course 6S099 on artificial gene...  \n"
     ]
    }
   ],
   "source": [
    "def agrupar_oraciones(oraciones, bloque_size=200):\n",
    "    bloques = []\n",
    "    for i in range(0, len(oraciones), bloque_size):\n",
    "        bloque = \" \".join(oraciones[i:i+bloque_size])  # Concatenar oraciones del bloque\n",
    "        bloques.append(bloque)\n",
    "    return bloques\n",
    "\n",
    "# Aplicamos la función y expandimos los bloques en un nuevo DataFrame\n",
    "from itertools import chain\n",
    "\n",
    "df['bloques'] = df['text'].apply(lambda texto: agrupar_oraciones(texto.split('.'), bloque_size=200))\n",
    "\n",
    "bloques_df = pd.DataFrame(\n",
    "    list(chain.from_iterable(df.apply(lambda row: [\n",
    "        (row['id'], row['guest'], row['title'], bloque) for bloque in row['bloques']\n",
    "    ], axis=1))),\n",
    "    columns=['id', 'guest', 'title', 'bloque_texto']\n",
    ")\n",
    "\n",
    "print(bloques_df.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c25dcc1c-1685-4294-89f4-628706e02273",
   "metadata": {},
   "source": [
    "### 3. Tokenización de los bloques de texto  \n",
    "Usamos `word_tokenize` de la librería NLTK para dividir cada bloque de texto (`bloque_texto`) en palabras o tokens individuales.  \n",
    "La tokenización se aplica a cada fila del DataFrame `bloques_df` y los resultados se almacenan en una nueva columna llamada `tokens`.  \n",
    "Se muestra una vista previa de los primeros bloques tokenizados, junto con el `id`, el `guest` y el `title` del episodio.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2551daf0-763c-4536-81ff-d58d3df41a2d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Saitama\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   id          guest          title  \\\n",
      "0   1    Max Tegmark       Life 3.0   \n",
      "1   1    Max Tegmark       Life 3.0   \n",
      "2   1    Max Tegmark       Life 3.0   \n",
      "3   1    Max Tegmark       Life 3.0   \n",
      "4   2  Christof Koch  Consciousness   \n",
      "\n",
      "                                              tokens  \n",
      "0  [As, part, of, MIT, course, 6S099, ,, Artifici...  \n",
      "1  [The, way, they, get, AGI, is, building, a, qu...  \n",
      "2  [So, there, 's, ,, it, 's, almost, like, a, se...  \n",
      "3  [And, that, kind, of, information, helps, to, ...  \n",
      "4  [As, part, of, MIT, course, 6S099, on, artific...  \n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "nltk.download('punkt')\n",
    "\n",
    "bloques_df['tokens'] = bloques_df['bloque_texto'].apply(word_tokenize)\n",
    "\n",
    "print(bloques_df[['id', 'guest', 'title', 'tokens']].head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c895a346-2602-4f4d-8d38-629189c45257",
   "metadata": {},
   "source": [
    "### 4. Entrenamiento del modelo Word2Vec  \n",
    "Entrenamos un modelo Word2Vec utilizando las listas de tokens generadas en el paso anterior.  \n",
    "- **`sentences=bloques_df['tokens']`**: El modelo se entrena con los tokens de cada bloque.  \n",
    "- **`vector_size=100`**: Cada palabra será representada por un vector de 100 dimensiones.  \n",
    "- **`window=5`**: El modelo considera un contexto de 5 palabras antes y después del término actual.  \n",
    "- **`min_count=1`**: Se incluyen todas las palabras, incluso aquellas que aparecen solo una vez.  \n",
    "El modelo aprende relaciones semánticas entre palabras basadas en el contexto dentro de los bloques de texto.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9b053f2b-019e-4d94-8eb8-944ac2af731f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "\n",
    "model = Word2Vec(sentences=bloques_df['tokens'], vector_size=100, window=5, min_count=1, workers=4)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f496de7-03be-4ea8-82e5-264af4321c59",
   "metadata": {},
   "source": [
    "### 5. Cálculo de los embeddings para cada bloque de texto  \n",
    "Para cada bloque de texto tokenizado, se genera un *embedding* que representa el significado promedio de todas las palabras del bloque.  \n",
    "- **`obtener_embedding(bloque_tokens, modelo)`**: Obtiene los vectores de las palabras y calcula el promedio para generar un único vector representativo del bloque.  \n",
    "- Si un bloque no tiene palabras conocidas por el modelo, se retorna un vector de ceros.  \n",
    "Los resultados se almacenan en la columna `embeddings` del DataFrame, y se muestra una vista previa de los primeros bloques con sus respectivos embeddings.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "21fdf5a6-21d2-4fba-bc69-537349e131cf",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   id          guest          title  \\\n",
      "0   1    Max Tegmark       Life 3.0   \n",
      "1   1    Max Tegmark       Life 3.0   \n",
      "2   1    Max Tegmark       Life 3.0   \n",
      "3   1    Max Tegmark       Life 3.0   \n",
      "4   2  Christof Koch  Consciousness   \n",
      "\n",
      "                                          embeddings  \n",
      "0  [-0.49504837, -0.45915484, 0.1681212, 0.111881...  \n",
      "1  [-0.52780443, -0.46918842, 0.10328533, 0.10727...  \n",
      "2  [-0.5186225, -0.44692662, 0.13203557, 0.075770...  \n",
      "3  [-0.48111993, -0.4711618, 0.21049581, 0.142526...  \n",
      "4  [-0.42403653, -0.45269567, 0.1766183, 0.001626...  \n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def obtener_embedding(bloque_tokens, modelo):\n",
    "    vectores = [modelo.wv[word] for word in bloque_tokens if word in modelo.wv]\n",
    "    if len(vectores) > 0:\n",
    "        return np.mean(vectores, axis=0)  # Promediar los vectores\n",
    "    else:\n",
    "        return np.zeros(modelo.vector_size)\n",
    "\n",
    "bloques_df['embeddings'] = bloques_df['tokens'].apply(lambda x: obtener_embedding(x, model))\n",
    "\n",
    "print(bloques_df[['id', 'guest', 'title', 'embeddings']].head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecef46c8-69aa-46b6-9d90-4a39201f7e5c",
   "metadata": {},
   "source": [
    "### 6. Reducción de dimensionalidad usando PCA  \n",
    "Se aplica **Análisis de Componentes Principales (PCA)** para reducir la dimensionalidad de los embeddings generados en el paso anterior.  \n",
    "- **`n_components=50`**: Los vectores de embeddings originales se reducen a 50 dimensiones para optimizar la eficiencia computacional sin perder demasiada información.  \n",
    "- **`pca.fit_transform()`**: Ajusta y transforma los embeddings para obtener una representación de menor dimensión.  \n",
    "El resultado es una matriz con los embeddings reducidos, lista para ser usada en el proceso de agrupamiento.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "79e412c2-e280-4e17-ab01-c236e5a06ea6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "pca = PCA(n_components=50)\n",
    "embeddings_reducidos = pca.fit_transform(np.array(bloques_df['embeddings'].tolist()))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40031480-7226-4dc4-a4ab-facb238323d6",
   "metadata": {},
   "source": [
    "### 7. Agrupamiento de los bloques usando MiniBatchKMeans  \n",
    "Se agrupan los bloques de texto en clústeres temáticos utilizando el algoritmo **MiniBatchKMeans**.  \n",
    "- **`n_clusters=5`**: Se especifica que el algoritmo debe crear 5 grupos temáticos.  \n",
    "- **`batch_size=1000`**: El algoritmo procesa los datos en pequeños lotes de 1000 bloques para mejorar la eficiencia en grandes volúmenes de datos.  \n",
    "- **`random_state=42`**: Se fija la semilla para garantizar resultados reproducibles.  \n",
    "Los clústeres asignados a cada bloque se almacenan en la columna `cluster` del DataFrame. Se muestra una vista previa de los primeros bloques con sus respectivos clústeres.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "84f93a97-12ea-4131-95af-9ed537ac6f50",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   id          guest          title  cluster\n",
      "0   1    Max Tegmark       Life 3.0        1\n",
      "1   1    Max Tegmark       Life 3.0        1\n",
      "2   1    Max Tegmark       Life 3.0        1\n",
      "3   1    Max Tegmark       Life 3.0        1\n",
      "4   2  Christof Koch  Consciousness        4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\InstallApps2023\\ANACONDA\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1930: FutureWarning: The default value of `n_init` will change from 3 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  super()._check_params_vs_input(X, default_n_init=3)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.cluster import MiniBatchKMeans\n",
    "\n",
    "n_clusters = 5\n",
    "kmeans = MiniBatchKMeans(n_clusters=n_clusters, batch_size=1000, random_state=42)\n",
    "bloques_df['cluster'] = kmeans.fit_predict(embeddings_reducidos)\n",
    "\n",
    "print(bloques_df[['id', 'guest', 'title', 'cluster']].head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26b80485-d5bc-4b1d-b7cf-951abbab6869",
   "metadata": {},
   "source": [
    "### 8. Selección de bloques representativos por clúster  \n",
    "Se seleccionan los bloques más representativos dentro de cada clúster basado en la distancia euclidiana al centroide del clúster.  \n",
    "- **`seleccionar_bloques_representativos(embeddings, centroide, n=3)`**: Calcula la distancia de cada bloque al centroide y selecciona los `n` bloques más cercanos.  \n",
    "- **`kmeans.cluster_centers_`**: Obtiene los centroides calculados por el algoritmo de agrupamiento.  \n",
    "- Para cada clúster, se extraen los bloques representativos y se almacenan en un nuevo DataFrame (`representativos_df`).  \n",
    "Se muestra una vista previa de los bloques representativos seleccionados junto con su `id`, `guest`, `title` y `cluster`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e1dd8932-33ef-45d7-947c-c68135f38f4a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     id             guest                                              title  \\\n",
      "0   303        Steve Keen                 Marxism, Capitalism, and Economics   \n",
      "1   193          Rob Reid  The Existential Threat of Engineered Viruses a...   \n",
      "2   254  Jay Bhattacharya                         The Case Against Lockdowns   \n",
      "3   306     Oriol Vinyals  Deep Learning and Artificial General Intelligence   \n",
      "4   181    Sergey Nazarov    Chainlink, Smart Contracts, and Oracle Networks   \n",
      "5    44    David Ferrucci  IBM Watson, Jeopardy & Deep Conversations with AI   \n",
      "6    88    Eric Weinstein  Geometric Unity and the Call for New Ideas, Le...   \n",
      "7   169         Ryan Hall         Solving Martial Arts from First Principles   \n",
      "8   259       Thomas Tull  From Batman Dark Knight Trilogy to AI and the ...   \n",
      "9   223    Travis Stevens               Judo, Olympics, and Mental Toughness   \n",
      "10  272     Brett Johnson                       US Most Wanted Cybercriminal   \n",
      "11  125         Ryan Hall  Martial Arts and the Philosophy of Violence, P...   \n",
      "12  221     Douglas Lenat  Cyc and the Quest to Solve Common Sense Reason...   \n",
      "13  314        Liv Boeree  Poker, Game Theory, AI, Simulation, Aliens & E...   \n",
      "14  252         Elon Musk  SpaceX, Mars, Tesla Autopilot, Self-Driving, R...   \n",
      "\n",
      "                                         bloque_texto  cluster  \n",
      "0    So how do we get onto Aussie football in Aust...        0  \n",
      "1    The easier one to manage, although it wouldn'...        0  \n",
      "2    And I mean, I jumped at the chance because I ...        0  \n",
      "3    And there's a tension between becoming the ma...        1  \n",
      "4   The following is a conversation with Sergei Na...        1  \n",
      "5    Learning is fundamental  And I think we're al...        1  \n",
      "6    Why was that? And it seems to be that they ha...        2  \n",
      "7    So that's that dopamine rush  So we want to s...        2  \n",
      "8   The following is a conversation with Thomas Ta...        2  \n",
      "9    Oh, there's an old school double leg  I forgo...        3  \n",
      "10   We'd take, you know, we'd steal stuff for her...        3  \n",
      "11   And I heard that the other day  I thought tha...        3  \n",
      "12   Well, but that's like, this goes to the white...        4  \n",
      "13   I'll be very ill informed in terms of making ...        4  \n",
      "14   There's just a lot of incredible things that ...        4  \n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics.pairwise import euclidean_distances\n",
    "\n",
    "def seleccionar_bloques_representativos(embeddings, centroide, n=3):\n",
    "    distancias = euclidean_distances(embeddings, [centroide])\n",
    "    indices_mas_cercanos = np.argsort(distancias.ravel())[:n]\n",
    "    return indices_mas_cercanos\n",
    "\n",
    "centroides = kmeans.cluster_centers_\n",
    "\n",
    "representativos = []\n",
    "for i in range(n_clusters):\n",
    "    indices_cl = bloques_df[bloques_df['cluster'] == i].index\n",
    "    embeddings_cl = np.array(embeddings_reducidos[indices_cl])\n",
    "    indices_representativos = seleccionar_bloques_representativos(embeddings_cl, centroides[i])\n",
    "    representativos.extend(bloques_df.iloc[indices_cl].iloc[indices_representativos].to_dict('records'))\n",
    "\n",
    "representativos_df = pd.DataFrame(representativos)\n",
    "print(representativos_df[['id', 'guest', 'title', 'bloque_texto', 'cluster']])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dabc0159-0fa9-4ace-9c76-024ac9e9aac8",
   "metadata": {},
   "source": [
    "### 9. Búsqueda de bloques relevantes basados en una consulta  \n",
    "Se implementa un sistema de búsqueda semántica para encontrar los bloques más relevantes en función de una consulta dada.  \n",
    "- **`obtener_embedding_busqueda(clave, modelo)`**: Convierte la consulta de texto en un *embedding* utilizando el modelo Word2Vec.  \n",
    "- **`cosine_similarity()`**: Calcula la similitud coseno entre el *embedding* de la consulta y los *embeddings* de los bloques de texto.  \n",
    "- Los valores de similitud se almacenan en la columna `similitudes` del DataFrame.  \n",
    "- Se seleccionan y ordenan los `top_n` bloques más relevantes y se muestra el resultado con su `id`, `guest`, `title`, texto del bloque, clúster asignado y nivel de similitud.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ba1698ce-4847-4741-ad6c-fb4e48ab0ef9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       id                guest  \\\n",
      "1833  292         Robin Hanson   \n",
      "7       3        Steven Pinker   \n",
      "37     13        Tomaso Poggio   \n",
      "3       1          Max Tegmark   \n",
      "33     11  Juergen Schmidhuber   \n",
      "\n",
      "                                                  title  \\\n",
      "1833  Alien Civilizations, UFOs, and the Future of H...   \n",
      "7                               AI in the Age of Reason   \n",
      "37                          Brains, Minds, and Machines   \n",
      "3                                              Life 3.0   \n",
      "33             Godel Machines, Meta-Learning, and LSTMs   \n",
      "\n",
      "                                           bloque_texto  cluster  similitudes  \n",
      "1833   So we discount the future in terms of caring ...        0     0.276640  \n",
      "7     You've studied the human mind, cognition, lang...        1     0.268079  \n",
      "37    The following is a conversation with Tommaso P...        1     0.266727  \n",
      "3      And that kind of information helps to build t...        1     0.261361  \n",
      "33     That's the current sort of profit in AI and t...        1     0.256135  \n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "def buscar_bloques_relevantes(clave, bloques_df, modelo, top_n=5):\n",
    "    embedding_consulta = obtener_embedding_busqueda(clave, modelo)\n",
    "    embeddings_bloques = np.array(bloques_df['embeddings'].tolist())\n",
    "    similitudes = cosine_similarity([embedding_consulta], embeddings_bloques)[0]\n",
    "    \n",
    "    bloques_df['similitudes'] = similitudes\n",
    "    indices_top = np.argsort(similitudes)[-top_n:][::-1]\n",
    "    return bloques_df.iloc[indices_top][['id', 'guest', 'title', 'bloque_texto', 'cluster', 'similitudes']]\n",
    "\n",
    "# Ejemplo de búsqueda\n",
    "consulta = \"inteligencia artificial\"\n",
    "resultados = buscar_bloques_relevantes(consulta, bloques_df, model)\n",
    "print(resultados)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
